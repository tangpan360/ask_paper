# å¤šæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

## é¡¹ç›®æ¦‚è¿°

å¤šæ–‡æ¡£é—®ç­”ç³»ç»Ÿæ˜¯åœ¨ç°æœ‰å•æ–‡æ¡£é—®ç­”ç³»ç»ŸåŸºç¡€ä¸Šçš„æ‰©å±•ï¼Œæ”¯æŒå¯¹å¤šç¯‡æ–‡ç« è¿›è¡Œç»Ÿä¸€æ£€ç´¢å’Œæ™ºèƒ½é—®ç­”ã€‚ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ¯ä¸ªæ¨¡å—å¯ç‹¬ç«‹æ›¿æ¢å’Œä¼˜åŒ–ï¼Œç¡®ä¿ç³»ç»Ÿçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚

## é¡¹ç›®ç»“æ„

```
/
â”œâ”€â”€ app.py                  # ä¸»å…¥å£æ–‡ä»¶
â”œâ”€â”€ pages/                  # Streamlitå¤šé¡µé¢
â”‚   â”œâ”€â”€ 00_ç™»å½•æ³¨å†Œ.py      # ç”¨æˆ·ç™»å½•æ³¨å†Œé¡µé¢
â”‚   â”œâ”€â”€ 01_ä¸Šä¼ æ–‡æ¡£.py      # PDFä¸Šä¼ å’Œå¤„ç†é¡µé¢
â”‚   â”œâ”€â”€ 02_æ„å»ºç´¢å¼•.py      # æŸ¥çœ‹æ–‡æ¡£å’Œæ„å»ºç´¢å¼•é¡µé¢  
â”‚   â”œâ”€â”€ 03_è®ºæ–‡é—®ç­”.py      # é—®ç­”é¡µé¢
â”‚   â”œâ”€â”€ 04_ç®¡ç†ä¸­å¿ƒ.py      # ç®¡ç†å‘˜ç®¡ç†ä¸­å¿ƒé¡µé¢
â”‚   â””â”€â”€ 05_å¤šæ–‡æ¡£é—®ç­”.py    # ã€æ–°å¢ã€‘å¤šæ–‡æ¡£é—®ç­”é¡µé¢
â”œâ”€â”€ src/                    # æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ auth.py             # ç”¨æˆ·è®¤è¯å’Œç®¡ç†
â”‚   â”œâ”€â”€ admin.py            # ç®¡ç†å‘˜åŠŸèƒ½
â”‚   â”œâ”€â”€ pdf_processor.py    # PDFå¤„ç†ç›¸å…³åŠŸèƒ½
â”‚   â”œâ”€â”€ build_index.py      # ç´¢å¼•æ„å»ºåŠŸèƒ½
â”‚   â”œâ”€â”€ retriever.py        # æ£€ç´¢å’Œé—®ç­”åŠŸèƒ½
â”‚   â”œâ”€â”€ utils.py            # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ multi_docs/         # ã€æ–°å¢ã€‘å¤šæ–‡æ¡£é—®ç­”æ¨¡å—
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ document_processor/  # æ–‡æ¡£å¤„ç†æ¨¡å—
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ loader.py        # æ–‡æ¡£åŠ è½½å™¨
â”‚       â”‚   â”œâ”€â”€ cleaner.py       # æ–‡æ¡£æ¸…æ´—å™¨
â”‚       â”‚   â””â”€â”€ splitter.py      # æ–‡æ¡£åˆ†å—å™¨
â”‚       â”œâ”€â”€ indexer/            # ç´¢å¼•æ„å»ºæ¨¡å—
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ vector_indexer.py  # å‘é‡ç´¢å¼•æ„å»ºå™¨
â”‚       â”‚   â”œâ”€â”€ sparse_indexer.py  # ç¨€ç–ç´¢å¼•æ„å»ºå™¨
â”‚       â”‚   â””â”€â”€ index_manager.py   # ç´¢å¼•ç®¡ç†å™¨
â”‚       â”œâ”€â”€ retriever/          # æ£€ç´¢æ¨¡å—
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ vector_retriever.py # å‘é‡æ£€ç´¢å™¨
â”‚       â”‚   â”œâ”€â”€ sparse_retriever.py # ç¨€ç–æ£€ç´¢å™¨
â”‚       â”‚   â””â”€â”€ hybrid_retriever.py # æ··åˆæ£€ç´¢å™¨
â”‚       â”œâ”€â”€ ranker/             # æ’åºæ¨¡å—
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base_ranker.py    # åŸºç¡€æ’åºå™¨
â”‚       â”‚   â””â”€â”€ reranker.py       # é‡æ’å™¨
â”‚       â”œâ”€â”€ generator/          # ç”Ÿæˆæ¨¡å—
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ prompt_builder.py # æç¤ºè¯æ„å»ºå™¨
â”‚       â”‚   â””â”€â”€ llm_generator.py  # å¤§æ¨¡å‹ç”Ÿæˆå™¨
â”‚       â””â”€â”€ pipeline/           # æµæ°´çº¿æ¨¡å—
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ rag_pipeline.py   # RAGæµæ°´çº¿
â”‚           â””â”€â”€ config.py         # é…ç½®ç®¡ç†
â”œâ”€â”€ data/                   # å­˜å‚¨ä¸Šä¼ çš„PDFæ–‡ä»¶
â”œâ”€â”€ output/                 # å­˜å‚¨å¤„ç†åçš„PDFè¾“å‡º
â”œâ”€â”€ storage/                # å­˜å‚¨ç´¢å¼•æ–‡ä»¶
â”‚   â””â”€â”€ multi_docs/         # ã€æ–°å¢ã€‘å¤šæ–‡æ¡£ç´¢å¼•å­˜å‚¨
â”‚       â”œâ”€â”€ global_index/   # å…¨å±€ç´¢å¼•æ˜ å°„
â”‚       â””â”€â”€ doc_indices/    # æ–‡æ¡£ç´¢å¼•å­˜å‚¨
â”œâ”€â”€ db/                     # æ•°æ®åº“æ–‡ä»¶
â”œâ”€â”€ config/                 # ç³»ç»Ÿé…ç½®ç›®å½•
â”‚   â””â”€â”€ multi_docs_config.yaml # ã€æ–°å¢ã€‘å¤šæ–‡æ¡£é…ç½®æ–‡ä»¶
â””â”€â”€ requirements.txt        # ä¾èµ–åº“åˆ—è¡¨
```

## åŠŸèƒ½æ¦‚è¿°

1. **å¤šæ–‡æ¡£ç®¡ç†**ï¼šæ”¯æŒå¤šç¯‡æ–‡æ¡£çš„ä¸Šä¼ ã€å¤„ç†å’Œç´¢å¼•æ„å»º
2. **è·¨æ–‡æ¡£æ£€ç´¢**ï¼šæ”¯æŒåœ¨å¤šç¯‡æ–‡æ¡£ä¸­è¿›è¡Œç»Ÿä¸€æ£€ç´¢
3. **æ™ºèƒ½æ’åº**ï¼šå¯¹æ£€ç´¢ç»“æœè¿›è¡Œæ™ºèƒ½æ’åºå’Œç­›é€‰
4. **ä¸Šä¸‹æ–‡ç”Ÿæˆ**ï¼šåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆé«˜è´¨é‡å›ç­”
5. **æºæ–‡æœ¬å¼•ç”¨**ï¼šæä¾›å›ç­”çš„æºæ–‡æœ¬å¼•ç”¨å’Œè¿½è¸ª

## æŠ€æœ¯æ¶æ„

ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—ï¼š

1. **æ–‡æ¡£å¤„ç†æ¨¡å—**ï¼šè´Ÿè´£æ–‡æ¡£è¯»å–ã€æ¸…æ´—å’Œåˆ†å—
2. **ç´¢å¼•æ„å»ºæ¨¡å—**ï¼šè´Ÿè´£ä¸ºæ–‡æ¡£æ„å»ºæ£€ç´¢ç´¢å¼•
3. **æ£€ç´¢æ¨¡å—**ï¼šè´Ÿè´£ä»ç´¢å¼•ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
4. **æ’åºæ¨¡å—**ï¼šè´Ÿè´£å¯¹æ£€ç´¢ç»“æœè¿›è¡Œæ’åºå’Œç­›é€‰
5. **ç”Ÿæˆæ¨¡å—**ï¼šè´Ÿè´£åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”
6. **æµæ°´çº¿æ¨¡å—**ï¼šè´Ÿè´£ç»„ç»‡å’Œåè°ƒå„ä¸ªæ¨¡å—çš„å·¥ä½œ

## å®æ–½è®¡åˆ’

### é˜¶æ®µä¸€ï¼šåŸºç¡€æ¶æ„æ­å»º

- [x] åˆ›å»ºé¡¹ç›®è§„åˆ’å’ŒREADME.md
- [ ] åˆ›å»ºå¤šæ–‡æ¡£é—®ç­”æ¨¡å—ç›®å½•ç»“æ„
  - [ ] åˆ›å»ºä¸»ç›®å½•å’Œå­ç›®å½•
  - [ ] è®¾ç½®åŸºç¡€é…ç½®æ–‡ä»¶
- [ ] å®ç°æ–‡æ¡£å¤„ç†æ¨¡å—
  - [ ] å®ç°æ–‡æ¡£åŠ è½½å™¨
  ```python
  def load_documents(file_paths):
      """åŠ è½½å¤šä¸ªæ–‡æ¡£"""
      documents = []
      for file_path in file_paths:
          document = load_document(file_path)
          documents.append(document)
      return documents
  ```
  - [ ] å®ç°æ–‡æ¡£æ¸…æ´—å™¨
  ```python
  def clean_document(document):
      """æ¸…æ´—æ–‡æ¡£å†…å®¹"""
      # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ã€æ ¼å¼åŒ–æ–‡æœ¬ç­‰
      cleaned_content = remove_special_chars(document.content)
      document.content = cleaned_content
      return document
  ```
  - [ ] å®ç°æ–‡æ¡£åˆ†å—å™¨
  ```python
  def split_document(document, chunk_size=1024, chunk_overlap=200):
      """å°†æ–‡æ¡£åˆ†å‰²æˆå—"""
      chunks = []
      # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ†å‰²æ–‡æœ¬
      for i in range(0, len(document.content), chunk_size - chunk_overlap):
          chunk = document.content[i:i + chunk_size]
          chunks.append(chunk)
      return chunks
  ```

### é˜¶æ®µäºŒï¼šç´¢å¼•æ„å»ºæ¨¡å—å¼€å‘

- [ ] å®ç°å‘é‡ç´¢å¼•æ„å»ºå™¨
  ```python
  def build_vector_index(chunks, embedding_model):
      """æ„å»ºå‘é‡ç´¢å¼•"""
      embeddings = []
      for chunk in chunks:
          embedding = embedding_model.embed_text(chunk)
          embeddings.append(embedding)
      return VectorIndex(chunks, embeddings)
  ```
- [ ] å®ç°ç¨€ç–ç´¢å¼•æ„å»ºå™¨
  ```python
  def build_sparse_index(chunks):
      """æ„å»ºç¨€ç–ç´¢å¼•ï¼ˆBM25ï¼‰"""
      tokenized_chunks = [tokenize(chunk) for chunk in chunks]
      bm25_index = BM25Index(tokenized_chunks)
      return bm25_index
  ```
- [ ] å®ç°ç´¢å¼•ç®¡ç†å™¨
  ```python
  class IndexManager:
      def __init__(self, storage_path):
          self.storage_path = storage_path
          self.indices = {}
          
      def add_index(self, doc_id, vector_index, sparse_index):
          """æ·»åŠ æ–‡æ¡£ç´¢å¼•"""
          self.indices[doc_id] = {
              "vector_index": vector_index,
              "sparse_index": sparse_index
          }
          self._save_index(doc_id)
          
      def load_indices(self, doc_ids):
          """åŠ è½½å¤šä¸ªæ–‡æ¡£çš„ç´¢å¼•"""
          loaded_indices = {}
          for doc_id in doc_ids:
              if doc_id not in self.indices:
                  self._load_index(doc_id)
              loaded_indices[doc_id] = self.indices[doc_id]
          return loaded_indices
  ```

### é˜¶æ®µä¸‰ï¼šæ£€ç´¢å’Œæ’åºæ¨¡å—å¼€å‘

- [ ] å®ç°å‘é‡æ£€ç´¢å™¨
  ```python
  def vector_retrieve(query, vector_index, top_k=10):
      """å‘é‡æ£€ç´¢ç›¸å…³æ–‡æ¡£å—"""
      query_embedding = embed_text(query)
      scores = compute_similarity(query_embedding, vector_index.embeddings)
      top_indices = get_top_k_indices(scores, top_k)
      return [vector_index.chunks[i] for i in top_indices]
  ```
- [ ] å®ç°ç¨€ç–æ£€ç´¢å™¨
  ```python
  def sparse_retrieve(query, sparse_index, top_k=10):
      """ç¨€ç–æ£€ç´¢ç›¸å…³æ–‡æ¡£å—"""
      query_tokens = tokenize(query)
      scores = sparse_index.get_scores(query_tokens)
      top_indices = get_top_k_indices(scores, top_k)
      return [sparse_index.chunks[i] for i in top_indices]
  ```
- [ ] å®ç°æ··åˆæ£€ç´¢å™¨
  ```python
  def hybrid_retrieve(query, vector_index, sparse_index, top_k=10):
      """æ··åˆæ£€ç´¢ç­–ç•¥"""
      vector_results = vector_retrieve(query, vector_index, top_k)
      sparse_results = sparse_retrieve(query, sparse_index, top_k)
      merged_results = merge_results(vector_results, sparse_results)
      return merged_results[:top_k]
  ```
- [ ] å®ç°é‡æ’å™¨
  ```python
  def rerank(query, chunks, model, top_k=5):
      """é‡æ’æ£€ç´¢ç»“æœ"""
      inputs = prepare_rerank_inputs(query, chunks)
      scores = model.predict(inputs)
      reranked_chunks = [chunk for _, chunk in sorted(
          zip(scores, chunks), key=lambda x: x[0], reverse=True
      )]
      return reranked_chunks[:top_k]
  ```

### é˜¶æ®µå››ï¼šç”Ÿæˆæ¨¡å—å¼€å‘

- [ ] å®ç°æç¤ºè¯æ„å»ºå™¨
  ```python
  def build_prompt(query, chunks):
      """æ„å»ºæç¤ºè¯"""
      context = "\n\n".join([f"æ–‡æ¡£{i+1}: {chunk}" for i, chunk in enumerate(chunks)])
      prompt = f"""ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä¸‹ï¼š
      ----------
      {context}
      ----------
      è¯·åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯è€Œä¸æ˜¯è‡ªå·±çš„çŸ¥è¯†ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯æ²¡æœ‰ç›¸å…³çŸ¥è¯†ï¼Œå¯ä»¥å›ç­”ä¸ç¡®å®šï¼š
      {query}
      """
      return prompt
  ```
- [ ] å®ç°å¤§æ¨¡å‹ç”Ÿæˆå™¨
  ```python
  def generate_answer(prompt, model):
      """ç”Ÿæˆå›ç­”"""
      response = model.generate(prompt)
      return response.text
  ```
- [ ] å®ç°æºæ–‡æœ¬å¼•ç”¨
  ```python
  def find_references(answer, chunks):
      """æŸ¥æ‰¾å›ç­”ä¸­çš„æºæ–‡æœ¬å¼•ç”¨"""
      references = []
      for chunk in chunks:
          if has_overlap(answer, chunk):
              references.append(chunk)
      return references
  ```

### é˜¶æ®µäº”ï¼šæµæ°´çº¿æ¨¡å—å¼€å‘

- [ ] å®ç°é…ç½®ç®¡ç†
  ```python
  def load_config(config_path):
      """åŠ è½½é…ç½®æ–‡ä»¶"""
      with open(config_path, 'r') as f:
          config = yaml.safe_load(f)
      return config
  ```
- [ ] å®ç°RAGæµæ°´çº¿
  ```python
  class RAGPipeline:
      def __init__(self, config):
          self.config = config
          self.index_manager = IndexManager(config["storage_path"])
          # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
          
      def process(self, query, doc_ids):
          """å¤„ç†æŸ¥è¯¢"""
          # 1. åŠ è½½ç´¢å¼•
          indices = self.index_manager.load_indices(doc_ids)
          
          # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
          chunks = []
          for doc_id, index in indices.items():
              doc_chunks = hybrid_retrieve(
                  query, 
                  index["vector_index"], 
                  index["sparse_index"],
                  self.config["retrieve_top_k"]
              )
              chunks.extend(doc_chunks)
          
          # 3. é‡æ’åº
          reranked_chunks = rerank(
              query, 
              chunks, 
              self.reranker,
              self.config["rerank_top_k"]
          )
          
          # 4. æ„å»ºæç¤ºè¯
          prompt = build_prompt(query, reranked_chunks)
          
          # 5. ç”Ÿæˆå›ç­”
          answer = generate_answer(prompt, self.llm)
          
          # 6. æŸ¥æ‰¾å¼•ç”¨
          references = find_references(answer, reranked_chunks)
          
          return {
              "answer": answer,
              "references": references,
              "chunks": reranked_chunks
          }
  ```

### é˜¶æ®µå…­ï¼šUIå¼€å‘å’Œç³»ç»Ÿé›†æˆ

- [ ] åˆ›å»ºå¤šæ–‡æ¡£é—®ç­”é¡µé¢
  ```python
  # pages/05_å¤šæ–‡æ¡£é—®ç­”.py
  import streamlit as st
  from src.multi_docs.pipeline import RAGPipeline
  from src.utils import get_user_documents
  
  # é¡µé¢è®¾ç½®
  st.set_page_config(page_title="å¤šæ–‡æ¡£é—®ç­”", page_icon="ğŸ“š", layout="wide")
  
  # æ£€æŸ¥ç™»å½•çŠ¶æ€
  if "user_id" not in st.session_state:
      st.error("è¯·å…ˆç™»å½•")
      st.stop()
      
  # åˆå§‹åŒ–RAGæµæ°´çº¿
  if "multi_doc_pipeline" not in st.session_state:
      config = load_config("config/multi_docs_config.yaml")
      st.session_state.multi_doc_pipeline = RAGPipeline(config)
  
  # è·å–ç”¨æˆ·æ–‡æ¡£
  user_docs = get_user_documents(st.session_state.user_id)
  indexed_docs = [doc for doc in user_docs if is_document_indexed(st.session_state.user_id, doc["doc_id"])]
  
  # æ–‡æ¡£é€‰æ‹©
  selected_docs = st.multiselect(
      "é€‰æ‹©è¦é—®ç­”çš„æ–‡æ¡£",
      options=[doc["filename"] for doc in indexed_docs],
      default=[]
  )
  
  # è·å–é€‰ä¸­æ–‡æ¡£çš„ID
  selected_doc_ids = [
      doc["doc_id"] for doc in indexed_docs 
      if doc["filename"] in selected_docs
  ]
  
  # é—®ç­”ç•Œé¢
  if selected_doc_ids:
      query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")
      if query:
          with st.spinner("æ­£åœ¨æ€è€ƒ..."):
              result = st.session_state.multi_doc_pipeline.process(
                  query, 
                  st.session_state.user_id, 
                  selected_doc_ids
              )
              
          st.write("### å›ç­”")
          st.write(result["answer"])
          
          with st.expander("æŸ¥çœ‹å¼•ç”¨"):
              for i, ref in enumerate(result["references"]):
                  st.write(f"**å¼•ç”¨ {i+1}**:")
                  st.write(ref)
  else:
      st.info("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ–‡æ¡£è¿›è¡Œé—®ç­”")
  ```
- [ ] é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿ
  - [ ] æ›´æ–°å¯¼èˆªèœå•
  - [ ] æ·»åŠ å¤šæ–‡æ¡£é—®ç­”å…¥å£
  - [ ] å…±äº«ç”¨æˆ·è®¤è¯å’Œæ•°æ®å­˜å‚¨

## å¤šæ–‡æ¡£ç´¢å¼•å­˜å‚¨ç»“æ„

å¤šæ–‡æ¡£ç´¢å¼•é‡‡ç”¨åˆ†å±‚å­˜å‚¨ç»“æ„ï¼Œç¡®ä¿é«˜æ•ˆçš„æ£€ç´¢å’Œç®¡ç†ï¼š

```
storage/
  multi_docs/
    global_index/
      metadata.json         # å…¨å±€å…ƒæ•°æ®
      doc_mapping.json      # æ–‡æ¡£æ˜ å°„è¡¨
    doc_indices/
      {user_id}/
        {doc_id_1}/
          vector_index/     # å‘é‡ç´¢å¼•
          sparse_index/     # ç¨€ç–ç´¢å¼•
          metadata.json     # æ–‡æ¡£å…ƒæ•°æ®
        {doc_id_2}/
          ...
```

## é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
# config/multi_docs_config.yaml

# æ–‡æ¡£å¤„ç†é…ç½®
document_processor:
  chunk_size: 1024
  chunk_overlap: 200
  split_method: "sentence"  # sentence, paragraph, fixed

# ç´¢å¼•é…ç½®
indexer:
  vector_model: "text-embedding-v3"
  sparse_type: "bm25"  # bm25, tf-idf
  use_hybrid: true

# æ£€ç´¢é…ç½®
retriever:
  retrieve_top_k: 10
  fusion_strategy: "rrf"  # rrf, simple_merge

# æ’åºé…ç½®
ranker:
  reranker_model: "bge-reranker-v2"
  rerank_top_k: 5

# ç”Ÿæˆé…ç½®
generator:
  llm_model: "gpt-4.1-mini"
  max_tokens: 1024
  temperature: 0.7
  system_prompt: "ä½ æ˜¯åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„AIåŠ©æ‰‹ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜æ—¶åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ã€‚å¦‚æœé—®é¢˜ä¸ä¸Šä¸‹æ–‡æ–‡æ¡£æ— å…³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚"

# ç³»ç»Ÿé…ç½®
system:
  storage_path: "storage/multi_docs"
  cache_size: 10  # ç¼“å­˜çš„æ–‡æ¡£æ•°é‡
```

## æ ¸å¿ƒç±»å›¾

```mermaid
classDiagram
    class DocumentProcessor {
        +load_document(file_path)
        +clean_document(document)
        +split_document(document, chunk_size, chunk_overlap)
        +process_document(file_path)
    }
    
    class IndexBuilder {
        +build_vector_index(chunks, embedding_model)
        +build_sparse_index(chunks)
        +build_indices(chunks)
    }
    
    class IndexManager {
        -indices
        +add_index(doc_id, vector_index, sparse_index)
        +get_index(doc_id)
        +load_indices(doc_ids)
        +remove_index(doc_id)
    }
    
    class Retriever {
        +vector_retrieve(query, vector_index, top_k)
        +sparse_retrieve(query, sparse_index, top_k)
        +hybrid_retrieve(query, vector_index, sparse_index, top_k)
    }
    
    class Ranker {
        +rerank(query, chunks, model, top_k)
    }
    
    class Generator {
        +build_prompt(query, chunks)
        +generate_answer(prompt, model)
        +find_references(answer, chunks)
    }
    
    class RAGPipeline {
        -index_manager
        -retriever
        -ranker
        -generator
        +process(query, user_id, doc_ids)
    }
    
    RAGPipeline --> IndexManager
    RAGPipeline --> Retriever
    RAGPipeline --> Ranker
    RAGPipeline --> Generator
    IndexManager --> IndexBuilder
    IndexBuilder --> DocumentProcessor
```

## æµç¨‹å›¾

```mermaid
flowchart TD
    A[ç”¨æˆ·æé—®] --> B[é€‰æ‹©æ–‡æ¡£]
    B --> C[åŠ è½½æ–‡æ¡£ç´¢å¼•]
    C --> D[æ£€ç´¢ç›¸å…³æ–‡æ¡£å—]
    D --> E[é‡æ’åºæ£€ç´¢ç»“æœ]
    E --> F[æ„å»ºæç¤ºè¯]
    F --> G[ç”Ÿæˆå›ç­”]
    G --> H[æŸ¥æ‰¾æºæ–‡æœ¬å¼•ç”¨]
    H --> I[å±•ç¤ºå›ç­”å’Œå¼•ç”¨]
    
    subgraph ç´¢å¼•æ„å»ºæµç¨‹
    J[ä¸Šä¼ æ–‡æ¡£] --> K[æ–‡æ¡£å¤„ç†]
    K --> L[æ–‡æ¡£åˆ†å—]
    L --> M[æ„å»ºå‘é‡ç´¢å¼•]
    L --> N[æ„å»ºç¨€ç–ç´¢å¼•]
    M --> O[ä¿å­˜ç´¢å¼•]
    N --> O
    end
```

## ä½¿ç”¨æ–¹æ³•

### 1. ç´¢å¼•æ„å»º

```python
from src.multi_docs.document_processor import DocumentProcessor
from src.multi_docs.indexer import IndexBuilder, IndexManager

# åˆå§‹åŒ–ç»„ä»¶
processor = DocumentProcessor(config)
builder = IndexBuilder(config)
manager = IndexManager(config["storage_path"])

# å¤„ç†æ–‡æ¡£
document = processor.load_document("path/to/document.pdf")
clean_document = processor.clean_document(document)
chunks = processor.split_document(clean_document)

# æ„å»ºç´¢å¼•
vector_index, sparse_index = builder.build_indices(chunks)

# ä¿å­˜ç´¢å¼•
manager.add_index("doc_123", vector_index, sparse_index)
```

### 2. å¤šæ–‡æ¡£æ£€ç´¢å’Œé—®ç­”

```python
from src.multi_docs.pipeline import RAGPipeline

# åˆå§‹åŒ–RAGæµæ°´çº¿
pipeline = RAGPipeline(config)

# å¤„ç†æŸ¥è¯¢
result = pipeline.process(
    "ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ?",
    user_id="user_123",
    doc_ids=["doc_1", "doc_2", "doc_3"]
)

# è·å–ç»“æœ
answer = result["answer"]
references = result["references"]
```

## å¼€å‘æ³¨æ„äº‹é¡¹

1. **æ¨¡å—åŒ–è®¾è®¡**ï¼š
   - æ¯ä¸ªæ¨¡å—åº”è¯¥æœ‰æ˜ç¡®çš„æ¥å£å’ŒèŒè´£
   - ç¡®ä¿æ¨¡å—ä¹‹é—´çš„ä½è€¦åˆæ€§ï¼Œä¾¿äºæ›¿æ¢å’Œå‡çº§

2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å¯¹å¤§å‹æ–‡æ¡£é›†åˆï¼Œè€ƒè™‘ä½¿ç”¨å¼‚æ­¥å¤„ç†
   - å®ç°ç´¢å¼•ç¼“å­˜æœºåˆ¶ï¼Œå‡å°‘é‡å¤åŠ è½½
   - è€ƒè™‘ä½¿ç”¨æ‰¹å¤„ç†æé«˜æ£€ç´¢æ•ˆç‡

3. **ç”¨æˆ·ä½“éªŒ**ï¼š
   - æä¾›è¿›åº¦åé¦ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ç´¢å¼•æ„å»ºå’Œæ£€ç´¢è¿‡ç¨‹ä¸­
   - æ”¯æŒæµå¼è¾“å‡ºå›ç­”ï¼Œæé«˜å“åº”é€Ÿåº¦
   - æä¾›æ¸…æ™°çš„æºæ–‡æœ¬å¼•ç”¨ï¼Œå¢å¼ºå¯ä¿¡åº¦

4. **é”™è¯¯å¤„ç†**ï¼š
   - å®ç°å¥å£®çš„é”™è¯¯å¤„ç†æœºåˆ¶
   - æä¾›å‹å¥½çš„é”™è¯¯æç¤º
   - æ”¯æŒè‡ªåŠ¨é‡è¯•å’Œæ¢å¤

## æµ‹è¯•è®¡åˆ’

1. **å•å…ƒæµ‹è¯•**ï¼š
   - æµ‹è¯•å„ä¸ªæ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½
   - éªŒè¯æ¥å£ä¸€è‡´æ€§

2. **é›†æˆæµ‹è¯•**ï¼š
   - æµ‹è¯•æ¨¡å—é—´çš„äº¤äº’
   - éªŒè¯æµæ°´çº¿çš„ç«¯åˆ°ç«¯åŠŸèƒ½

3. **æ€§èƒ½æµ‹è¯•**ï¼š
   - æµ‹è¯•å¤§è§„æ¨¡æ–‡æ¡£é›†çš„ç´¢å¼•æ„å»ºæ€§èƒ½
   - æµ‹è¯•å¤šæ–‡æ¡£æ£€ç´¢çš„å“åº”æ—¶é—´

4. **ç”¨æˆ·æµ‹è¯•**ï¼š
   - æ”¶é›†ç”¨æˆ·åé¦ˆ
   - ä¼˜åŒ–ç”¨æˆ·ç•Œé¢å’Œäº¤äº’ä½“éªŒ

## æ‰©å±•è®¡åˆ’

1. **æ”¯æŒæ›´å¤šæ–‡æ¡£æ ¼å¼**ï¼š
   - Wordæ–‡æ¡£
   - HTMLç½‘é¡µ
   - è¡¨æ ¼æ•°æ®

2. **é«˜çº§æ£€ç´¢åŠŸèƒ½**ï¼š
   - è¯­ä¹‰è¿‡æ»¤
   - å…ƒæ•°æ®æœç´¢
   - æ—¶é—´èŒƒå›´ç­›é€‰

3. **ä¸ªæ€§åŒ–é…ç½®**ï¼š
   - ç”¨æˆ·åå¥½è®¾ç½®
   - æ£€ç´¢ç­–ç•¥è‡ªå®šä¹‰
   - å›ç­”é£æ ¼è°ƒæ•´

4. **å¤šè¯­è¨€æ”¯æŒ**ï¼š
   - æ‰©å±•åˆ°å…¶ä»–è¯­è¨€çš„æ–‡æ¡£å¤„ç†
   - å¤šè¯­è¨€é—®ç­”æ”¯æŒ

## å‚è€ƒèµ„æº

- [LlamaIndexæ–‡æ¡£](https://docs.llamaindex.ai/)
- [Sentence Transformers](https://www.sbert.net/)
- [BM25ç®—æ³•ä»‹ç»](https://en.wikipedia.org/wiki/Okapi_BM25)
- [RAGæœ€ä½³å®è·µ](https://www.pinecone.io/learn/retrieval-augmented-generation/) 